\documentclass[a4paper, 14pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{subfiles}
\usepackage{ dsfont }
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float} % Utilisation strict du positionnement des images
\author{Tuo Ismaël Maurice}
\date{14/04/2025}
\begin{document}
	\section{Definition}
	Similarity by cosine determines the degree of similarity between two sentences by calculating the cosine of the angle formed by the two vectors in each sentence.\\
	Consider the following sentences:
	\begin{enumerate}
		\item Bonjour John.\footnote{'Hello John.' in French}
		\item Bonjour Doé.\footnote{'Hello Doé.' in French}
	\end{enumerate}
	Note that the two sentences are similar, but how can you tell with a computer?

	\section{Simple 2-dimensional space example}
	Consider the following sentences:
	\begin{enumerate}
		\item Bonjour John.
		\item Bonjour.\footnote{'Hello.' in French}
	\end{enumerate}
	Note that one of the two sentences is made up of two words, so they can be represented in a two-dimensional space.\\
	The following similarity table is created:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Sentences} & \textbf{Bonjour} & \textbf{John} \\
			\hline
			Bonjour John & 1 & 1 \\
			Bonjour & 1 & 0 \\
			\hline
		\end{tabular}
		\caption{Simple similarity table example 1}
		\label{tab:exemplesimple}
	\end{table}
	The representation in a reference frame gives:
	 \begin{figure}[H]
		\includegraphics[scale=0.5, width=15cm]{./img/vecteur_exemple_simple.png}
		\caption{Simple representation similarity example 1}
	\end{figure}
	Sentence 1 is represented by the vector $A_{1}(1, 1)$ and sentence 2 by the vector $B_{2}(1, 0)$.\\
	The angle formed by these two vectors is the $\alpha$ angle, whose cosine is used to calculate the similarity between the two sentences.\\
	In the previous example, $\alpha=45^{o}$ of which $cos(\alpha)=0.71$. So $71\%$ chance that the two sentences are similar.\\
	Suppose sentence 2 is \textbf{Bonjour, Bonjour, Bonjour.} then we get:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Sentences} & \textbf{Bonjour} & \textbf{John} \\
			\hline
			Bonjour John & 1 & 1 \\
			Bonjour, Bonjour, Bonjour  & 3 & 0 \\
			\hline
		\end{tabular}
	\caption{Simple similarity table example 2}
	\label{tab:exemplesimple}
	\end{table}
	The representation in a reference frame gives:
	\begin{figure}[H]
		\includegraphics[scale=0.5, width=15cm]{./img/vecteur_exemple_simple_example2.png}
		\caption{Simple representation similarity example 2}
	\end{figure}
	\textbf{Regardless of the length of sentence 2, the number of times \textit{Bonjour} is added does not change the cosine of $\alpha$.}
    \paragraph{Summary:} Cosine similarity is the cosine of the angle between two vectors, revealing the similarity between them. Its value is always between 0 and 1.

    \section{Case of exact similarity}
    Consider the following two sentences:
    \begin{enumerate}
    	\item Bonjour John
    	\item Bonjour John
    \end{enumerate}
    It's like a table of similarities:
    \begin{table}[H]
    	\centering
    	\begin{tabular}{|c|c|c|}
    		\hline
    		\textbf{Sentences} & \textbf{Bonjour} & \textbf{John} \\
    		\hline
    		Bonjour John & 1 & 1 \\
    		Bonjour John  & 1 & 1 \\
    		\hline
    	\end{tabular}
    	\caption{Exact similarity table}
    	\label{tab:similariteexacte}
    \end{table}
    \newpage
    The representation in a reference frame gives:
    \begin{figure}[H]
    	\includegraphics[scale=0.5, width=15cm]{./img/vecteur_similitude_exacte.png}
    	\caption{Exact similarity representation}
    \end{figure}
    The two vectors coincide, so $\alpha = 0^{o}$ or $cos(\alpha)=1$, i.e. 100\% chance that the two vectors are similar. We can conclude that the two sentences are totally similar.

    \section{No similarity}
    Consider the following two sentences:
    \begin{enumerate}
    	\item Bonjour.
    	\item John.
    \end{enumerate}
	The similarity table gives:
	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Sentences} & \textbf{Bonjour} & \textbf{John} \\
			\hline
			Bonjour & 1 & 0 \\
			John  & 0 & 1 \\
			\hline
		\end{tabular}
		\caption{Table no similarities}
		\label{tab:nonsimilarite}
	\end{table}
	\newpage
	The representation in a reference frame gives:
	\begin{figure}[H]
		\includegraphics[scale=0.5, width=15cm]{./img/vecteur_non_similitude.png}
		\caption{Non-similarity representation}
	\end{figure}
	The angle formed by the two vectors is $\alpha = 90^{o}$ so $cos(\alpha) = 0$ or a 0\% chance that the two vectors are similar. We can conclude that the two sentences are completely different.

    \section{Summary}
	  \[
		cos(\alpha) =
		\begin{cases}
			0&\text{If there is no similarity between the two sentences.}\\
			1 & \text{Exact similarity.}\\
			\in ]0, 1[ & \text{Sensibly similar when both sentences have words in common.}
		\end{cases}
	  \]
		To get the  \textbf{cosinus} of the similarity between two sentences, follow these steps:
		\begin{enumerate}
			\item Make a word frequency chart (count the number of occurrences of each word in each sentence).
			\item Show points.
			\item Find the angle between Vectors.
			\item Calculate the cosine of the formed angle.
		\end{enumerate}

    \section{Complex cases}
    The previous cases work for sentences with two words, i.e. representable in a 2-dimensional space $\mathds{R} \text{x} \mathds{R}$. \\
    Real-life sentences are either multi-word or representable in a $\mathds{R}^{n}$ space, in which case we use the following formula:

   	\[
   		cos(\alpha) = \frac{\sum_{i=1}^{n}A_{i}B{i}}{\sqrt{\sum_{i=1}^{n}A_{i}^{2}} * \sqrt{\sum_{i=1}^{n}B_{i}^{2}}}, avec \ i = \  \text{word index}
   	\]
   	Consider the following sentences:
   	\begin{enumerate}
   		\item Bonjour John
   		\item Bonjour
   	\end{enumerate}
   	 \begin{table}[H]
   		\centering
   		\begin{tabular}{|c|c|c|}
   			\hline
   			\textbf{Sentences} & \textbf{Bonjour} & \textbf{John} \\
   			\hline
   			Bonjour John & 1 & 1 \\
   			Bonjour  & 1 & 0 \\
   			\hline
   		\end{tabular}
   		\caption{Simple similarity table example 1}
   		\label{tab:similitude}
   	\end{table}
   	Calculating the similarity gives:
   	\[
   		cos(\alpha) = \frac{(1*1) + (1*0)}{\sqrt{1^{2} + 1^{2}} * \sqrt{1^{2} + 0^{2}}} = \frac{1}{\sqrt{2} * 1} = 0.7071
   	\]
   	The $cos(45)$ found previously.\\
   	Let's consider two sentences:
  	\begin{enumerate}
  		\item Bonjour tout le monde.
  		\item Bonjour John
  	\end{enumerate}
  	The similarity table gives:
  	\begin{table}[H]
  		\centering
  		\begin{tabular}{|c|c|c|c|c|c|}
  			\hline
  			\textbf{Sentences} & \textbf{Bonjour} & \textbf{tout} & \textbf{le} & \textbf{monde} & \textbf{John}\\
  			\hline
  			Bonjour tout le monde. & 1 & 1 & 1 & 1 & 0 \\
  			Bonjour  John & 1 & 0 & 0 & 0 & 1 \\
  			\hline
  		\end{tabular}
  		\caption{Multidimensional similarity table}
  		\label{tab:similitudecomplexe}
  	\end{table}
	Calculating the similarity gives:
	\[
		cos(\alpha) = \frac{(1*1) + (1*0) + (1*0) + (1*0) + (0*1)}{\sqrt{1^{2} + 1^{2} +  1^{2} +  1^{2} + 0^{2}} * \sqrt{1^{2} + 0^{2} + 0^{2} + 0^{2} + 1^{2}}} = \frac{1}{2 * \sqrt{2}} = 0.35
	\]
	A 35\% chance of being similar.

    \section{Go further}
    The problem with this method is that similarity is based on word construction (syntactic/grammatical level), so two words \textbf{Hi} and \textbf{Hello} will be considered dissimilar because the implicit meaning is not taken into account.\\
    Similarly, \textbf{better} and \textbf{good} will be considered as not similar..\\
    In projects, we use the word lemma, i.e. the basic form of the word, to calculate similarity.
	Using the lemma, we get:
    \begin{enumerate}
    	\item[\textbullet] better => lemma good
    	\item[\textbullet] good => lemma good
    \end{enumerate}
    Using the lemmas \textbf{better} et \textbf{good} will be represented by their lemmatized form and therefore similar.\\
    Furthermore, this method does not take into account the semantic meaning between two sentences, i.e. two sentences \textbf{The cat eats the mouse} and \textbf{The mouse eats the cat} will be considered syntactically and grammatically similar, but semantically (meaning-wise) they are totally different.\\
    To take the subject further, you can use NLP (Natural Language Processing) libraries such as \href{https://spacy.io/}{SpaCy} and \href{https://www.nltk.org/}{NLTK}, which contain automatic language processing tools.\\
	Code examples can be found here:  \href{https://github.com/tisma95/articles/tree/master/cosine-similarity/sentence-similarities-benchmark}{Calculating similarity in different ways} which shows various calculation methods natively with Python, SpaCy, NLTK.
\end{document}